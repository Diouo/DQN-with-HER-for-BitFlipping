{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode_Experience():\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, goal):\n",
    "        self.memory += [(state, action, reward, next_state, done, goal)]\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitFlip(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 10\n",
    "    }\n",
    "\n",
    "    def __init__(self, bit_length=50, max_steps=50):\n",
    "        super(BitFlip, self).__init__()\n",
    "\n",
    "        if bit_length is None or bit_length < 1:\n",
    "            raise ValueError('bit_length must be >= 1, found {}'.format(bit_length))\n",
    "        else:\n",
    "            self.bit_length = bit_length\n",
    "        \n",
    "        if max_steps is None or max_steps < 1:\n",
    "            self.max_steps = bit_length\n",
    "        else:\n",
    "            self.max_steps = max_steps\n",
    "\n",
    "        self.action_space = spaces.Discrete(bit_length)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'state': spaces.Box(low=0, high=1, shape=(bit_length, )),\n",
    "            'goal': spaces.Box(low=0, high=1, shape=(bit_length, )),\n",
    "        })\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _terminate(self):\n",
    "        return np.array_equal(self.state, self.goal) or self.steps >= self.max_steps\n",
    "\n",
    "    def _reward(self):\n",
    "        return -1 if (self.state != self.goal).any() else 0\n",
    "\n",
    "    def _step(self, action):\n",
    "        self.state[action] = 1-self.state[action]\n",
    "        self.steps += 1\n",
    "\n",
    "        return self.state, self._reward(), self._terminate()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.steps = 0\n",
    "\n",
    "        self.state = np.random.randint(2, size=(self.bit_length))\n",
    "\n",
    "        # make sure goal is not the initial state\n",
    "        self.goal = self.state\n",
    "        while np.array_equal(self.state, self.goal):\n",
    "            self.goal = np.random.randint(2, size=(self.bit_length))\n",
    "\n",
    "        return self.state, self.goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, goal_size, model_out_path, clip_target_value=True):\n",
    "        self.state_size = state_size\n",
    "        self.goal_size = goal_size\n",
    "        self.action_size = action_size\n",
    "        self.clip_target_value = clip_target_value\n",
    "        self.memory = []\n",
    "\n",
    "        # hyperparameters for the agent and HER\n",
    "        # as same as the paper: https://arxiv.org/pdf/1707.01495.pdf 's Appendix A\n",
    "        self.epsilon = 0.2  # exploration\n",
    "        self.epsilon_min = 0.02  # min exploration\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.tau = 0.95  # target net update weight\n",
    "        self.gamma = 0.98\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-3\n",
    "        self.buffer_size = int(1e6)\n",
    "        self.hidden_size = 256\n",
    "\n",
    "        self.path = model_out_path\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.build_model()\n",
    "        self.writer = SummaryWriter(self.path + '/tensorboard')\n",
    "\n",
    "\n",
    "    # build the double DQN\n",
    "    def build_model(self):  \n",
    "        print('     Building double DQN')\n",
    "        \n",
    "        self.DQN_eval = DQN(input_size=self.state_size + self.goal_size, hidden_size=self.hidden_size, output_size=self.action_size).to(self.device).train()\n",
    "        self.DQN_target = DQN(input_size=self.state_size + self.goal_size, hidden_size=self.hidden_size, output_size=self.action_size).to(self.device).eval()\n",
    "        current_path = self.path + '/checkpoint' + '/init_model.pt'\n",
    "        torch.save(self.DQN_eval.state_dict(), current_path)\n",
    "        self.DQN_target.load_state_dict(torch.load(current_path))\n",
    "\n",
    "        self.criterion = nn.MSELoss().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.DQN_eval.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        \n",
    "    def choose_action(self, state, goal):\n",
    "        print('     Chooseing action')\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            act_values = self.DQN_eval(torch.concat((state,goal), 1))\n",
    "            return torch.argmax(act_values[0])  \n",
    "\n",
    "\n",
    "    def remember(self, ep_experience):\n",
    "        print('     Storing experience')\n",
    "        self.memory += ep_experience.memory\n",
    "        if len(self.memory) > self.buffer_size:\n",
    "            self.memory = self.memory[-self.buffer_size:]  # empty the first memories\n",
    "\n",
    "\n",
    "    def replay(self, epoch, optimization_steps):\n",
    "        print('     Replaying experience')\n",
    "\n",
    "        if len(self.memory) < self.batch_size:  # if there's no enough transitions, do nothing\n",
    "            print('     data in memory is too small')\n",
    "            return 0\n",
    "\n",
    "        losses = 0\n",
    "        for _ in range(optimization_steps):\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "            ss = minibatch[:, 0] # state\n",
    "            rs = minibatch[:, 2] # reward\n",
    "            nss = minibatch[:, 3] # next_state\n",
    "            ds = minibatch[:, 4] # done\n",
    "            gs =minibatch[:, 5] # goal\n",
    "\n",
    "            # Q_EVAL(st, at)\n",
    "            q_eval = torch.cat([ss, gs], dim=1).to(self.device)\n",
    "            q_eval = self.DQN_eval(q_eval) \n",
    "            pred =  torch.max(q_eval, dim=0)\n",
    "\n",
    "            # Q_TARGET(st+1, at+1)\n",
    "            q_target_next_state = torch.cat([nss, gs], dim=1).to(self.device)\n",
    "            q_target_next_state = self.DQN_target(q_target_next_state)\n",
    "            q_target_next_state = torch.max(q_target_next_state, dim=0)\n",
    "            target = rs + self.gamma * (1 - ds) * q_target_next_state\n",
    "            if self.clip_target_value:\n",
    "                target = torch.clamp(target, min=-1 / (1 - self.gamma), max=0)\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            mse_loss = self.criterion(pred, target)\n",
    "            mse_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses += mse_loss.item()\n",
    "\n",
    "        self.writer.add_scalar(tag=\"loss\", scalar_value=losses / optimization_steps, global_step=epoch)\n",
    "        self.writer.add_scalar(tag=\"lr\", scalar_value=self.optimizer.state_dict()['param_groups'][0]['lr'], global_step=epoch)\n",
    "\n",
    "\n",
    "    def update_target_net(self, epoch, decay=True):\n",
    "        \n",
    "        print('     Updating target DQN by EMA')\n",
    "        target_params = self.DQN_target.parameters()\n",
    "        eval_params = self.DQN_eval.parameters()\n",
    "        for t, e in zip(target_params, eval_params):\n",
    "            t =  self.tau * e + (1 - self.tau) * t\n",
    "        self.DQN_target.load_state_dict(target_params)\n",
    "\n",
    "        print('     Saving target DQN')\n",
    "        checkpoint={\n",
    "            'epoch':epoch,\n",
    "            'state_dict':self.DQN_target.state_dict(),\n",
    "            'optimizeG_state_dict':self.optimizer.state_dict(),\n",
    "                    }\n",
    "        checkpoint_out_path = self.path +'/checkpoint/'\n",
    "        torch.save(checkpoint, checkpoint_out_path + str(epoch) + '_checkpoint.pkl')\n",
    "        \n",
    "        if decay:\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/guozy/RL_Project/BitFlip.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m ep_experience_her \u001b[39m=\u001b[39m Episode_Experience()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m env \u001b[39m=\u001b[39m BitFlip(bit_length\u001b[39m=\u001b[39mn, max_steps\u001b[39m=\u001b[39mn)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(state_size\u001b[39m=\u001b[39;49mn, action_size\u001b[39m=\u001b[39;49mn, goal_size\u001b[39m=\u001b[39;49mn, clip_target_value\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m success_rate \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/guozy/RL_Project/BitFlip.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1e6\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_model()\n",
      "\u001b[1;32m/home/guozy/RL_Project/BitFlip.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_model\u001b[39m(\u001b[39mself\u001b[39m):  \u001b[39m# set value network\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     tf\u001b[39m.\u001b[39mreset_default_graph()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msess \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mSession()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.250.217.225/home/guozy/RL_Project/BitFlip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mplaceholder(tf\u001b[39m.\u001b[39mfloat32, [\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_size], \u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # to store different model information\n",
    "    now = datetime.datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    model_out_path = '/home/guozy/RL_Project/result/' + now\n",
    "    if os.path.exists(model_out_path) == False:\n",
    "        os.mkdir(model_out_path)\n",
    "    checkpoints_out_path = model_out_path +'/checkpoint/'\n",
    "    if os.path.exists(checkpoints_out_path) == False:\n",
    "        os.mkdir(checkpoints_out_path)\n",
    "    writer_out_path = model_out_path + '/tensorboard'\n",
    "    if os.path.exists(writer_out_path) == False:\n",
    "        os.mkdir(writer_out_path)\n",
    "    \n",
    "    # hyperparameters for the agent and HER\n",
    "    # as same as the paper: https://arxiv.org/pdf/1707.01495.pdf 's Appendix A\n",
    "    n = 50\n",
    "    num_epochs = 200\n",
    "    num_episodes = 16\n",
    "    optimization_steps = 40\n",
    "    K = 4  \n",
    "\n",
    "    ep_experience = Episode_Experience()\n",
    "    ep_experience_her = Episode_Experience()\n",
    "    env = BitFlip(bit_length=n, max_steps=n)\n",
    "    agent = Agent(state_size=n, action_size=n, goal_size=n, model_out_path=model_out_path, clip_target_value=True)\n",
    "\n",
    "    losses = []\n",
    "    success_rate = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\n===> Epoch {} starts'.format(epoch))\n",
    "        successes = 0\n",
    "\n",
    "        # step 1: get the dataset\n",
    "        for n in range(num_episodes):\n",
    "            state, goal = env._reset()\n",
    "\n",
    "            # step 1.1: get the original dataset\n",
    "            for t in range(n):\n",
    "                action = agent.choose_action([state], [goal])\n",
    "                next_state, reward, done = env._step(action)\n",
    "                ep_experience.add(state, action, reward, next_state, done, goal)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            successes += done\n",
    "\n",
    "            # step 1.2: get the HER dataset\n",
    "            for t in range(len(ep_experience.memory)):\n",
    "                for k in range(K):\n",
    "                    future = np.random.randint(t, len(ep_experience.memory))\n",
    "                    goal = ep_experience.memory[future][3]  # next_state of future\n",
    "                    state = ep_experience.memory[t][0]\n",
    "                    action = ep_experience.memory[t][1]\n",
    "                    next_state = ep_experience.memory[t][3]\n",
    "                    done = np.array_equal(next_state, goal)\n",
    "                    reward = 0 if done else -1\n",
    "                    ep_experience_her.add(state, action, reward, next_state, done, goal)\n",
    "\n",
    "            # step 1.3: put the datasets into the agent\n",
    "            agent.remember(ep_experience)\n",
    "            agent.remember(ep_experience_her)\n",
    "            ep_experience.clear()\n",
    "            ep_experience_her.clear()\n",
    "\n",
    "        # step 2: train the eval DQN\n",
    "        agent.replay(epoch=epoch, optimization_steps=optimization_steps)\n",
    "\n",
    "        # step 3: update and save the target DQN by EMA\n",
    "        agent.update_target_net(epoch=epoch, decay=True)\n",
    "\n",
    "        # step 4: store the history\n",
    "        agent.writer.add_scalar(tag=\"success rate\",scalar_value=successes / num_episodes, global_step=epoch)\n",
    "        agent.writer.add_scalar(tag=\"epsilon\",scalar_value=agent.epsilon, global_step=epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyan_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
