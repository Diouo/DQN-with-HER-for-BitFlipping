{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode_Experience():\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, goal):\n",
    "        self.memory += [(state, action, reward, next_state, done, goal)]\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitFlip():\n",
    "    def __init__(self, bit_length=50, max_steps=50):\n",
    "\n",
    "        if bit_length is None or bit_length < 1:\n",
    "            raise ValueError('bit_length must be >= 1, found {}'.format(bit_length))\n",
    "        else:\n",
    "            self.bit_length = bit_length\n",
    "        \n",
    "        if max_steps is None or max_steps < 1:\n",
    "            self.max_steps = bit_length\n",
    "        else:\n",
    "            self.max_steps = max_steps\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _terminate(self):\n",
    "        if np.array_equal(self.state, self.goal):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0  \n",
    "\n",
    "    def _reward(self):\n",
    "        return -1 if (self.state != self.goal).any() else 0\n",
    "\n",
    "    def _step(self, action):\n",
    "        change = np.zeros(len(self.state))\n",
    "        change[action] = 1\n",
    "        if self.state[action] == 0:\n",
    "            self.state = self.state + change\n",
    "        elif  self.state[action] == 1:\n",
    "            self.state = self.state - change\n",
    "            \n",
    "        self.steps += 1\n",
    "\n",
    "        return self.state, self._reward(), self._terminate()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.steps = 0\n",
    "\n",
    "        self.state = np.random.randint(2, size=(self.bit_length))\n",
    "\n",
    "        # make sure goal is not the initial state\n",
    "        self.goal = self.state\n",
    "        while np.array_equal(self.state, self.goal):\n",
    "            self.goal = np.random.randint(2, size=(self.bit_length))\n",
    "\n",
    "        return self.state, self.goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, goal_size, model_out_path, clip_target_value=True):\n",
    "        self.state_size = state_size\n",
    "        self.goal_size = goal_size\n",
    "        self.action_size = action_size\n",
    "        self.clip_target_value = clip_target_value\n",
    "        self.memory = []\n",
    "\n",
    "        # hyperparameters for the agent and HER\n",
    "        # as same as the paper: https://arxiv.org/pdf/1707.01495.pdf 's Appendix A\n",
    "        self.epsilon = 0.2  # exploration\n",
    "        self.epsilon_min = 0.02  # min exploration\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.tau = 0.95  # target net update weight\n",
    "        self.gamma = 0.98\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 1e-3\n",
    "        self.buffer_size = int(1e6)\n",
    "        self.hidden_size = 256\n",
    "\n",
    "        self.model_out_path = model_out_path\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.build_model()\n",
    "        self.writer = SummaryWriter(self.model_out_path + '/tensorboard')\n",
    "\n",
    "\n",
    "    # build the double DQN\n",
    "    def build_model(self):  \n",
    "        print('     Building double DQN')\n",
    "        \n",
    "        self.DQN_eval = DQN(input_size=self.state_size + self.goal_size, hidden_size=self.hidden_size, output_size=self.action_size).to(self.device).train()\n",
    "        self.DQN_target = DQN(input_size=self.state_size + self.goal_size, hidden_size=self.hidden_size, output_size=self.action_size).to(self.device).eval()\n",
    "        current_path = self.model_out_path + '/checkpoint' + '/init_model.pt'\n",
    "        torch.save(self.DQN_eval.state_dict(), current_path)\n",
    "        self.DQN_target.load_state_dict(torch.load(current_path))\n",
    "\n",
    "        self.criterion = nn.MSELoss().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.DQN_eval.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        \n",
    "    def choose_action(self, state, goal):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            goal =  torch.tensor(goal)\n",
    "            input = torch.concat((state,goal), 0).type(torch.FloatTensor).to(self.device)\n",
    "            act_values = self.DQN_eval(input) # tensor, len=50\n",
    "            \n",
    "            return torch.argmax(act_values).item() \n",
    "\n",
    "\n",
    "    def remember(self, ep_experience):\n",
    "        print('     Storing experience')\n",
    "        self.memory += ep_experience.memory\n",
    "        if len(self.memory) > self.buffer_size:\n",
    "            self.memory = self.memory[-self.buffer_size:]  # empty the first memories\n",
    "\n",
    "\n",
    "    def replay(self, epoch, optimization_steps):\n",
    "        print('\\n===> Replaying experience')\n",
    "\n",
    "        if len(self.memory) < self.batch_size:  # if there's no enough transitions, do nothing\n",
    "            print('\\n===> data in memory is too small')\n",
    "            return 0\n",
    "\n",
    "        losses = 0\n",
    "        for _ in range(optimization_steps):\n",
    "\n",
    "            # transform the list into multiple ndarrays\n",
    "            minibatch = random.sample(self.memory, self.batch_size) # list, len=128, element:state, action, reward, next_state, done, goal\n",
    "            ss = np.zeros((self.batch_size, self.state_size)) # state\n",
    "            rs = np.zeros(self.batch_size) # reward\n",
    "            nss = np.zeros((self.batch_size, self.state_size)) # next_state\n",
    "            ds = np.zeros(self.batch_size) # done\n",
    "            gs = np.zeros((self.batch_size, self.state_size)) # goal\n",
    "            for i in range(self.batch_size):\n",
    "                ss[i] = minibatch[i][0]\n",
    "                rs[i] = minibatch[i][2]\n",
    "                nss[i] = minibatch[i][3]\n",
    "                ds[i] = minibatch[i][4]\n",
    "                gs[i] = minibatch[i][5]\n",
    "\n",
    "            # Q_EVAL(st, at)\n",
    "            q_eval = torch.tensor(np.concatenate((ss, gs), axis=1)).type(torch.FloatTensor).to(self.device)\n",
    "            q_eval = self.DQN_eval(q_eval) \n",
    "            pred =  torch.max(q_eval, dim=1)[0] # tensor, len=128, all>=0\n",
    "            if self.clip_target_value:\n",
    "                pred = torch.clamp(pred, min=-1 / (1 - self.gamma), max=0) # tensor, len=128\n",
    "            # print(pred)\n",
    "            \n",
    "            # Q_TARGET(st+1, at+1)\n",
    "            q_target_next_state = torch.tensor(np.concatenate((nss, gs), axis=1)).type(torch.FloatTensor).to(self.device)\n",
    "            q_target_next_state = self.DQN_target(q_target_next_state)\n",
    "            q_target_next_state = torch.max(q_target_next_state, dim=1)[0] # tensor, len=128, all>=0\n",
    "            rs = torch.tensor(rs).type(torch.FloatTensor).to(self.device)\n",
    "            ds = torch.tensor(ds).type(torch.FloatTensor).to(self.device)\n",
    "            target = rs + self.gamma * (1 - ds) * q_target_next_state\n",
    "            if self.clip_target_value:\n",
    "                target = torch.clamp(target, min=-1 / (1 - self.gamma), max=0) # tensor, len=128\n",
    "            # print(target)\n",
    "\n",
    "            # train DQN eval\n",
    "            self.optimizer.zero_grad()\n",
    "            mse_loss = self.criterion(pred, target)\n",
    "            mse_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses += mse_loss.item()\n",
    "\n",
    "        self.writer.add_scalar(tag=\"loss\", scalar_value=losses / optimization_steps, global_step=epoch)\n",
    "        self.writer.add_scalar(tag=\"lr\", scalar_value=self.optimizer.state_dict()['param_groups'][0]['lr'], global_step=epoch)\n",
    "\n",
    "\n",
    "    def update_target_net(self, epoch, mode='RESET', decay=True):\n",
    "\n",
    "        if mode == 'EMA':\n",
    "            print('\\n===> Updating target DQN by EMA')\n",
    "            new_dict = {}\n",
    "            for key, t, e in zip(self.DQN_target.state_dict().keys(), self.DQN_target.state_dict().values(), self.DQN_eval.state_dict().values()):\n",
    "                new_dict[key] = self.tau * e + (1 - self.tau) * t\n",
    "            self.DQN_target.load_state_dict(new_dict)\n",
    "        elif mode == 'RESET':\n",
    "            print('\\n===> Updating target DQN by RESET')\n",
    "            self.DQN_target.load_state_dict(self.DQN_eval.state_dict())\n",
    "\n",
    "        print('\\n===> Saving target DQN')\n",
    "        checkpoint={\n",
    "            'epoch':epoch,\n",
    "            'net_state_dict':self.DQN_target.state_dict(),\n",
    "            'opt_state_dict':self.optimizer.state_dict(),\n",
    "                    }\n",
    "        checkpoint_out_path = self.model_out_path +'/checkpoint/'\n",
    "        torch.save(checkpoint, checkpoint_out_path + str(epoch) + '_checkpoint.pkl')\n",
    "        \n",
    "        if decay:\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (824708623.py, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [264], line 84\u001b[0;36m\u001b[0m\n\u001b[0;31m    agent.update_target_net(epoch=epoch, mode='RESET' decay=True)\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # to store different model information\n",
    "    current_path = os.getcwd()\n",
    "    result_path = current_path + '/result/'\n",
    "    if os.path.exists(result_path) == False:\n",
    "        os.mkdir(result_path)\n",
    "    now = datetime.datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    model_out_path =  result_path + now\n",
    "    if os.path.exists(model_out_path) == False:\n",
    "        os.mkdir(model_out_path)\n",
    "    checkpoints_out_path = model_out_path +'/checkpoint/'\n",
    "    if os.path.exists(checkpoints_out_path) == False:\n",
    "        os.mkdir(checkpoints_out_path)\n",
    "    writer_out_path = model_out_path + '/tensorboard'\n",
    "    if os.path.exists(writer_out_path) == False:\n",
    "        os.mkdir(writer_out_path)\n",
    "    \n",
    "    # hyperparameters for the agent and HER\n",
    "    n = 50\n",
    "    num_epochs = 800\n",
    "    num_episodes = 16\n",
    "    optimization_steps = 4\n",
    "    K = 1\n",
    "\n",
    "    ep_experience = Episode_Experience()\n",
    "    ep_experience_her = Episode_Experience()\n",
    "    env = BitFlip(bit_length=n, max_steps=n)\n",
    "    agent = Agent(state_size=n, action_size=n, goal_size=n, model_out_path=model_out_path, clip_target_value=False)\n",
    "\n",
    "    losses = []\n",
    "    success_rate = []\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        print('\\n===> Epoch {} starts'.format(epoch))\n",
    "        successes = 0\n",
    "        distances = 0\n",
    "\n",
    "        # step 1: get the dataset\n",
    "        for eposode in range(1, num_episodes+1):\n",
    "            print('\\n===> Episode {} starts'.format(eposode))\n",
    "            state, goal = env._reset() \n",
    "            done = 0\n",
    "\n",
    "            # step 1.1: get the original dataset\n",
    "            print('     Getting the original dataset')\n",
    "            for t in range(n):\n",
    "                action = agent.choose_action(state, goal)\n",
    "                next_state, reward, done = env._step(action) \n",
    "                distances += np.sum(np.abs(next_state-goal)) # check the progresss of DQN\n",
    "                ep_experience.add(state, action, reward, next_state, done, goal) # ndarray, int, int, ndarray, int, ndarray\n",
    "                state = next_state\n",
    "                if done == 1:\n",
    "                    successes += 1\n",
    "                    break\n",
    "\n",
    "            # step 1.2: get the HER dataset\n",
    "            print('     Getting the HER dataset')\n",
    "            for t in range(len(ep_experience.memory)):\n",
    "                for k in range(t,t+K):\n",
    "                    # choice = np.random.randint(t, len(ep_experience.memory))  # future strategy in HER\n",
    "                    choice = t # my strategy\n",
    "                    goal = ep_experience.memory[choice][3]  # next_state of future\n",
    "                    state = ep_experience.memory[t][0]\n",
    "                    action = ep_experience.memory[t][1]\n",
    "                    next_state = ep_experience.memory[t][3]\n",
    "                    # print(np.sum(np.abs(next_state-goal)))\n",
    "                    done = 1 if np.array_equal(next_state, goal) else 0\n",
    "                    reward = 0 if done else -1\n",
    "                    ep_experience_her.add(state, action, reward, next_state, done, goal)\n",
    "\n",
    "            # step 1.3: transfer the datasets to the agent\n",
    "            print('     Transferring the datasets to the agent')\n",
    "            agent.remember(ep_experience)\n",
    "            agent.remember(ep_experience_her)\n",
    "            ep_experience.clear()\n",
    "            ep_experience_her.clear()\n",
    "\n",
    "        # step 2: train the eval DQN\n",
    "        agent.replay(epoch=epoch, optimization_steps=optimization_steps)\n",
    "\n",
    "        # step 3: update and save the target DQN\n",
    "        agent.update_target_net(epoch=epoch, mode='RESET', decay=True)\n",
    "\n",
    "        # step 4: store the history\n",
    "        agent.writer.add_scalar(tag=\"distance\",scalar_value=distances / num_episodes / n, global_step=epoch)\n",
    "        agent.writer.add_scalar(tag=\"success rate\",scalar_value=successes / num_episodes, global_step=epoch)\n",
    "        agent.writer.add_scalar(tag=\"epsilon\",scalar_value=agent.epsilon, global_step=epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyan_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
